{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatModelクラスとLLMクラスの違い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='こんにちは！何かお手伝いできることはありますか？😊\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 12, 'total_token_count': 13, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 12}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.06469776233037312, 'model_name': 'gemini-2.0-flash-001'}, id='run-039762e0-083f-41ef-9327-280f8a5e7536-0', usage_metadata={'input_tokens': 1, 'output_tokens': 12, 'total_tokens': 13})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "chat_model = ChatVertexAI(project=\"\", model=\"gemini-2.0-flash-001\")\n",
    "chat_model.invoke(\"こんにちは\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(project=\"\", model=\"gemini-2.0-flash-001\") \n",
    "llm.invoke(\"こんにちは\")\n",
    "\n",
    "#Vertexai時の初期化パラメーターと異なり、GeminiAPI経由の場合はAPIキーがいりそう\n",
    "#ただし、ここのOutputはChatModelクラスと違い、ただの文字列が返ってくる。\n",
    "\n",
    "# 例: \"こんにちは！何かお手伝いできることはありますか？😊\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPromptTemplateとPromptTemplateの違い\n",
    "\n",
    "基本的な使い方\n",
    "\n",
    "https://python.langchain.com/docs/concepts/prompt_templates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "\n",
    "from_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='あなたは翻訳専門家です。japnease を english に翻訳する。', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n[ariticle]\\nこんにちは\\n', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# systemプロンプトとuserプロンプトの設定\n",
    "sys_prompt = \"あなたは翻訳専門家です。{source_language} を {target_language} に翻訳する。\"\n",
    "user_prompt = \"\"\"\n",
    "[ariticle]\n",
    "{article}\n",
    "\"\"\"\n",
    "\n",
    "# ChatPromptTemplate.from_messagesはBaseMessageを継承\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\", sys_prompt),\n",
    "    (\"human\", user_prompt),\n",
    "  ]\n",
    ")\n",
    "prompt.format_messages(source_language=\"japnease\", target_language=\"english\", article=\"こんにちは\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplateのインスタンス化\n",
    "\n",
    "MessagesPlaceholderは複数チャット履歴またはHumanMessagesを複数与えときに役に立つ -> マルチモーダル時にtextと一緒に与える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='あなたは翻訳専門家です。{source_language} を {target_language} に翻訳する。', additional_kwargs={}, response_metadata={}), HumanMessage(content='おはよう', additional_kwargs={}, response_metadata={}), HumanMessage(content='写真です', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# そのままインスタンス化する操作\n",
    "# 複数チャット履歴またはHumanMessagesを複数与える -> マルチモーダル時にtextと一緒に与える\n",
    "\n",
    "human_message_content = []\n",
    "human_message_content.append(HumanMessage(\"おはよう\"))\n",
    "human_message_content.append(HumanMessage(\"写真です\"))\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "  [\n",
    "    SystemMessage(sys_prompt),\n",
    "    MessagesPlaceholder(\"multimodalPlusUserInput\")\n",
    "  ],\n",
    "  input_variables = [\"source_language\", \"target_language\"],\n",
    ")\n",
    "\n",
    "# TODO: source_languageとtarget_languageがうまくフォーマットされていない問題がある\n",
    "# prompt.format_messages(source_language=\"japnease\", target_language=\"english\", multimodalPlusUserInput=human_message_content)\n",
    "prompt.invoke({\"source_language\":\"japanease\", \"target_language\":\"english\", \"multimodalPlusUserInput\": human_message_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'世界に関する笑い話を教えて下さい。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "  \"{name}に関する笑い話を教えて下さい。\"\n",
    ")\n",
    "\n",
    "prompt_template.format(name=\"世界\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praser の使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "答えてください。\n",
      " The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "``` \n",
      " ジョーク教えて下さい。 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Joke(BaseModel):\n",
    "  setup: str = Field(description=\"question\")\n",
    "  punchline: str = Field(description=\"answer\")\n",
    "\n",
    "joke_query = \"ジョーク教えて下さい。\"\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template = \"答えてください。\\n {format_instructions} \\n {query} \\n\",\n",
    "  # 毎回手動で与える必要のある変数\n",
    "  input_variables=[\"query\"],\n",
    "  # templateに組み込まれている変数\n",
    "  # get_format_instructionsはあくまでLangchain側で定義されているfew-short prompt\n",
    "  partial_variables={\"format_instructions\":parser.get_format_instructions}\n",
    ")\n",
    "\n",
    "print(prompt.format(query=joke_query))\n",
    "\n",
    "chain = prompt | chat_model | parser\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あ|る|と|こ|ろ|に|、|と|て|も|忘|れ|っ|ぽ|い|お|じ|い|さ|ん|が|い|ま|し|た|。|\n",
      "|\n",
      "|あ|る|日|、|お|じ|い|さ|ん|は|買|い|物|に|出|か|け|よ|う|と|し|ま|し|た|。|し|か|し|、|玄|関|で|立|ち|止|ま|り|、|何|を|買|う|か|全|く|思|い|出|せ|ま|せ|ん|。|\n",
      "|\n",
      "|困|っ|た|お|じ|い|さ|ん|は|、|奥|さ|ん|に|「|お|ば|あ|さ|ん|、|悪|い|ん|だ|け|ど|、|今|日|何|を|買|っ|て|く|る|ん|だ|っ|た|か|教|え|て|く|れ|な|い|か|？|」|と|尋|ね|ま|し|た|。|\n",
      "|\n",
      "|お|ば|あ|さ|ん|は|少|し|呆|れ|な|が|ら|も|「|あ|ら|、|ま|た|忘|れ|ち|ゃ|っ|た|の|？|い|い|わ|よ|。|卵|を|6|個|買|っ|て|き|て|ち|ょ|う|だ|い|。|も|し|パ|ン|屋|さ|ん|が|開|い|て|い|た|ら|、|パ|ン|も|6|個|買|っ|て|き|て|ね|」|と|言|い|ま|し|た|。|\n",
      "|\n",
      "|お|じ|い|さ|ん|は|何|度|も|「|卵|6|個|、|パ|ン|屋|さ|ん|が|あ|れ|ば|パ|ン|6|個|」|と|ブ|ツ|ブ|ツ|言|い|な|が|ら|家|を|出|ま|し|た|。|\n",
      "|\n",
      "|し|ば|ら|く|し|て|、|お|じ|い|さ|ん|は|買|い|物|か|ら|帰|っ|て|き|ま|し|た|。|し|か|し|、|手|に|持|っ|て|い|る|の|は|パ|ン|が|1|2|個|。|\n",
      "|\n",
      "|お|ば|あ|さ|ん|は|「|あ|ら|、|お|じ|い|さ|ん|。|卵|は|ど|こ|に|行|っ|た|の|？|そ|れ|に|、|ど|う|し|て|パ|ン|が|1|2|個|も|あ|る|の|？|」|と|尋|ね|ま|し|た|。|\n",
      "|\n",
      "|お|じ|い|さ|ん|は|ニ|ヤ|リ|と|笑|っ|て|「|パ|ン|屋|さ|ん|、|開|い|て|た|ん|だ|も|ん|！|」|\n",
      "|"
     ]
    }
   ],
   "source": [
    "sys_prompt = \"\"\n",
    "user_prompt = \"ジョークを教えて下さい、200文字以上500文字以内。\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "  [\n",
    "  SystemMessage(sys_prompt),\n",
    "  HumanMessage(user_prompt)\n",
    "  ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model\n",
    "\n",
    "# 文字一つずつの出力方法\n",
    "# TODO:もっといい方法ないかな?\n",
    "for chunk in chain.stream({\"input\": \"\"}):\n",
    "  for char in chunk.content:\n",
    "    print(char, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 1, 'output_tokens': 12, 'total_tokens': 13}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"こんにちは\").usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langsmith\n",
    "\n",
    "openaiのモデルしか対応してない?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='こんにちは！何かお手伝いできることはありますか？\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 11, 'total_token_count': 12, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 11}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.12914298881183972, 'model_name': 'gemini-2.0-flash-001'}, id='run-24618940-ab52-45e4-9665-3bf1464eca52-0', usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] =\"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] =\"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] =\"\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] =\"\"\n",
    "\n",
    "chat_model.invoke(\"こんにちは\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-SbMP_ClQ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
