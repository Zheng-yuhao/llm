{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatModelã‚¯ãƒ©ã‚¹ã¨LLMã‚¯ãƒ©ã‚¹ã®é•ã„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼ŸğŸ˜Š\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 12, 'total_token_count': 13, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 12}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.06469776233037312, 'model_name': 'gemini-2.0-flash-001'}, id='run-039762e0-083f-41ef-9327-280f8a5e7536-0', usage_metadata={'input_tokens': 1, 'output_tokens': 12, 'total_tokens': 13})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "chat_model = ChatVertexAI(project=\"\", model=\"gemini-2.0-flash-001\")\n",
    "chat_model.invoke(\"ã“ã‚“ã«ã¡ã¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(project=\"\", model=\"gemini-2.0-flash-001\") \n",
    "llm.invoke(\"ã“ã‚“ã«ã¡ã¯\")\n",
    "\n",
    "#Vertexaiæ™‚ã®åˆæœŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¨ç•°ãªã‚Šã€GeminiAPIçµŒç”±ã®å ´åˆã¯APIã‚­ãƒ¼ãŒã„ã‚Šãã†\n",
    "#ãŸã ã—ã€ã“ã“ã®Outputã¯ChatModelã‚¯ãƒ©ã‚¹ã¨é•ã„ã€ãŸã ã®æ–‡å­—åˆ—ãŒè¿”ã£ã¦ãã‚‹ã€‚\n",
    "\n",
    "# ä¾‹: \"ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼ŸğŸ˜Š\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPromptTemplateã¨PromptTemplateã®é•ã„\n",
    "\n",
    "åŸºæœ¬çš„ãªä½¿ã„æ–¹\n",
    "\n",
    "https://python.langchain.com/docs/concepts/prompt_templates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "\n",
    "from_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='ã‚ãªãŸã¯ç¿»è¨³å°‚é–€å®¶ã§ã™ã€‚japnease ã‚’ english ã«ç¿»è¨³ã™ã‚‹ã€‚', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n[ariticle]\\nã“ã‚“ã«ã¡ã¯\\n', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# systemãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨userãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­å®š\n",
    "sys_prompt = \"ã‚ãªãŸã¯ç¿»è¨³å°‚é–€å®¶ã§ã™ã€‚{source_language} ã‚’ {target_language} ã«ç¿»è¨³ã™ã‚‹ã€‚\"\n",
    "user_prompt = \"\"\"\n",
    "[ariticle]\n",
    "{article}\n",
    "\"\"\"\n",
    "\n",
    "# ChatPromptTemplate.from_messagesã¯BaseMessageã‚’ç¶™æ‰¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\", sys_prompt),\n",
    "    (\"human\", user_prompt),\n",
    "  ]\n",
    ")\n",
    "prompt.format_messages(source_language=\"japnease\", target_language=\"english\", article=\"ã“ã‚“ã«ã¡ã¯\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplateã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
    "\n",
    "MessagesPlaceholderã¯è¤‡æ•°ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¾ãŸã¯HumanMessagesã‚’è¤‡æ•°ä¸ãˆã¨ãã«å½¹ã«ç«‹ã¤ -> ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ™‚ã«textã¨ä¸€ç·’ã«ä¸ãˆã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='ã‚ãªãŸã¯ç¿»è¨³å°‚é–€å®¶ã§ã™ã€‚{source_language} ã‚’ {target_language} ã«ç¿»è¨³ã™ã‚‹ã€‚', additional_kwargs={}, response_metadata={}), HumanMessage(content='ãŠã¯ã‚ˆã†', additional_kwargs={}, response_metadata={}), HumanMessage(content='å†™çœŸã§ã™', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ãã®ã¾ã¾ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹æ“ä½œ\n",
    "# è¤‡æ•°ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¾ãŸã¯HumanMessagesã‚’è¤‡æ•°ä¸ãˆã‚‹ -> ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ™‚ã«textã¨ä¸€ç·’ã«ä¸ãˆã‚‹\n",
    "\n",
    "human_message_content = []\n",
    "human_message_content.append(HumanMessage(\"ãŠã¯ã‚ˆã†\"))\n",
    "human_message_content.append(HumanMessage(\"å†™çœŸã§ã™\"))\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "  [\n",
    "    SystemMessage(sys_prompt),\n",
    "    MessagesPlaceholder(\"multimodalPlusUserInput\")\n",
    "  ],\n",
    "  input_variables = [\"source_language\", \"target_language\"],\n",
    ")\n",
    "\n",
    "# TODO: source_languageã¨target_languageãŒã†ã¾ããƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚Œã¦ã„ãªã„å•é¡ŒãŒã‚ã‚‹\n",
    "# prompt.format_messages(source_language=\"japnease\", target_language=\"english\", multimodalPlusUserInput=human_message_content)\n",
    "prompt.invoke({\"source_language\":\"japanease\", \"target_language\":\"english\", \"multimodalPlusUserInput\": human_message_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä¸–ç•Œã«é–¢ã™ã‚‹ç¬‘ã„è©±ã‚’æ•™ãˆã¦ä¸‹ã•ã„ã€‚'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "  \"{name}ã«é–¢ã™ã‚‹ç¬‘ã„è©±ã‚’æ•™ãˆã¦ä¸‹ã•ã„ã€‚\"\n",
    ")\n",
    "\n",
    "prompt_template.format(name=\"ä¸–ç•Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praser ã®ä½¿ã„æ–¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      " The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "``` \n",
      " ã‚¸ãƒ§ãƒ¼ã‚¯æ•™ãˆã¦ä¸‹ã•ã„ã€‚ \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Joke(BaseModel):\n",
    "  setup: str = Field(description=\"question\")\n",
    "  punchline: str = Field(description=\"answer\")\n",
    "\n",
    "joke_query = \"ã‚¸ãƒ§ãƒ¼ã‚¯æ•™ãˆã¦ä¸‹ã•ã„ã€‚\"\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template = \"ç­”ãˆã¦ãã ã•ã„ã€‚\\n {format_instructions} \\n {query} \\n\",\n",
    "  # æ¯å›æ‰‹å‹•ã§ä¸ãˆã‚‹å¿…è¦ã®ã‚ã‚‹å¤‰æ•°\n",
    "  input_variables=[\"query\"],\n",
    "  # templateã«çµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹å¤‰æ•°\n",
    "  # get_format_instructionsã¯ã‚ãã¾ã§Langchainå´ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹few-short prompt\n",
    "  partial_variables={\"format_instructions\":parser.get_format_instructions}\n",
    ")\n",
    "\n",
    "print(prompt.format(query=joke_query))\n",
    "\n",
    "chain = prompt | chat_model | parser\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚|ã‚‹|ã¨|ã“|ã‚|ã«|ã€|ã¨|ã¦|ã‚‚|å¿˜|ã‚Œ|ã£|ã½|ã„|ãŠ|ã˜|ã„|ã•|ã‚“|ãŒ|ã„|ã¾|ã—|ãŸ|ã€‚|\n",
      "|\n",
      "|ã‚|ã‚‹|æ—¥|ã€|ãŠ|ã˜|ã„|ã•|ã‚“|ã¯|è²·|ã„|ç‰©|ã«|å‡º|ã‹|ã‘|ã‚ˆ|ã†|ã¨|ã—|ã¾|ã—|ãŸ|ã€‚|ã—|ã‹|ã—|ã€|ç„|é–¢|ã§|ç«‹|ã¡|æ­¢|ã¾|ã‚Š|ã€|ä½•|ã‚’|è²·|ã†|ã‹|å…¨|ã|æ€|ã„|å‡º|ã›|ã¾|ã›|ã‚“|ã€‚|\n",
      "|\n",
      "|å›°|ã£|ãŸ|ãŠ|ã˜|ã„|ã•|ã‚“|ã¯|ã€|å¥¥|ã•|ã‚“|ã«|ã€Œ|ãŠ|ã°|ã‚|ã•|ã‚“|ã€|æ‚ª|ã„|ã‚“|ã |ã‘|ã©|ã€|ä»Š|æ—¥|ä½•|ã‚’|è²·|ã£|ã¦|ã|ã‚‹|ã‚“|ã |ã£|ãŸ|ã‹|æ•™|ãˆ|ã¦|ã|ã‚Œ|ãª|ã„|ã‹|ï¼Ÿ|ã€|ã¨|å°‹|ã­|ã¾|ã—|ãŸ|ã€‚|\n",
      "|\n",
      "|ãŠ|ã°|ã‚|ã•|ã‚“|ã¯|å°‘|ã—|å‘†|ã‚Œ|ãª|ãŒ|ã‚‰|ã‚‚|ã€Œ|ã‚|ã‚‰|ã€|ã¾|ãŸ|å¿˜|ã‚Œ|ã¡|ã‚ƒ|ã£|ãŸ|ã®|ï¼Ÿ|ã„|ã„|ã‚|ã‚ˆ|ã€‚|åµ|ã‚’|6|å€‹|è²·|ã£|ã¦|ã|ã¦|ã¡|ã‚‡|ã†|ã |ã„|ã€‚|ã‚‚|ã—|ãƒ‘|ãƒ³|å±‹|ã•|ã‚“|ãŒ|é–‹|ã„|ã¦|ã„|ãŸ|ã‚‰|ã€|ãƒ‘|ãƒ³|ã‚‚|6|å€‹|è²·|ã£|ã¦|ã|ã¦|ã­|ã€|ã¨|è¨€|ã„|ã¾|ã—|ãŸ|ã€‚|\n",
      "|\n",
      "|ãŠ|ã˜|ã„|ã•|ã‚“|ã¯|ä½•|åº¦|ã‚‚|ã€Œ|åµ|6|å€‹|ã€|ãƒ‘|ãƒ³|å±‹|ã•|ã‚“|ãŒ|ã‚|ã‚Œ|ã°|ãƒ‘|ãƒ³|6|å€‹|ã€|ã¨|ãƒ–|ãƒ„|ãƒ–|ãƒ„|è¨€|ã„|ãª|ãŒ|ã‚‰|å®¶|ã‚’|å‡º|ã¾|ã—|ãŸ|ã€‚|\n",
      "|\n",
      "|ã—|ã°|ã‚‰|ã|ã—|ã¦|ã€|ãŠ|ã˜|ã„|ã•|ã‚“|ã¯|è²·|ã„|ç‰©|ã‹|ã‚‰|å¸°|ã£|ã¦|ã|ã¾|ã—|ãŸ|ã€‚|ã—|ã‹|ã—|ã€|æ‰‹|ã«|æŒ|ã£|ã¦|ã„|ã‚‹|ã®|ã¯|ãƒ‘|ãƒ³|ãŒ|1|2|å€‹|ã€‚|\n",
      "|\n",
      "|ãŠ|ã°|ã‚|ã•|ã‚“|ã¯|ã€Œ|ã‚|ã‚‰|ã€|ãŠ|ã˜|ã„|ã•|ã‚“|ã€‚|åµ|ã¯|ã©|ã“|ã«|è¡Œ|ã£|ãŸ|ã®|ï¼Ÿ|ã|ã‚Œ|ã«|ã€|ã©|ã†|ã—|ã¦|ãƒ‘|ãƒ³|ãŒ|1|2|å€‹|ã‚‚|ã‚|ã‚‹|ã®|ï¼Ÿ|ã€|ã¨|å°‹|ã­|ã¾|ã—|ãŸ|ã€‚|\n",
      "|\n",
      "|ãŠ|ã˜|ã„|ã•|ã‚“|ã¯|ãƒ‹|ãƒ¤|ãƒª|ã¨|ç¬‘|ã£|ã¦|ã€Œ|ãƒ‘|ãƒ³|å±‹|ã•|ã‚“|ã€|é–‹|ã„|ã¦|ãŸ|ã‚“|ã |ã‚‚|ã‚“|ï¼|ã€|\n",
      "|"
     ]
    }
   ],
   "source": [
    "sys_prompt = \"\"\n",
    "user_prompt = \"ã‚¸ãƒ§ãƒ¼ã‚¯ã‚’æ•™ãˆã¦ä¸‹ã•ã„ã€200æ–‡å­—ä»¥ä¸Š500æ–‡å­—ä»¥å†…ã€‚\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "  [\n",
    "  SystemMessage(sys_prompt),\n",
    "  HumanMessage(user_prompt)\n",
    "  ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model\n",
    "\n",
    "# æ–‡å­—ä¸€ã¤ãšã¤ã®å‡ºåŠ›æ–¹æ³•\n",
    "# TODO:ã‚‚ã£ã¨ã„ã„æ–¹æ³•ãªã„ã‹ãª?\n",
    "for chunk in chain.stream({\"input\": \"\"}):\n",
    "  for char in chunk.content:\n",
    "    print(char, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 1, 'output_tokens': 12, 'total_tokens': 13}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"ã“ã‚“ã«ã¡ã¯\").usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langsmith\n",
    "\n",
    "openaiã®ãƒ¢ãƒ‡ãƒ«ã—ã‹å¯¾å¿œã—ã¦ãªã„?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 11, 'total_token_count': 12, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 11}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.12914298881183972, 'model_name': 'gemini-2.0-flash-001'}, id='run-24618940-ab52-45e4-9665-3bf1464eca52-0', usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] =\"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] =\"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] =\"\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] =\"\"\n",
    "\n",
    "chat_model.invoke(\"ã“ã‚“ã«ã¡ã¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-SbMP_ClQ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
