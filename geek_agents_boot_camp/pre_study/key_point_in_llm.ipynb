{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Attention in LLM ?\n",
    "\n",
    "注意力机制就是一种方法帮助LLM去**抓住重点**而忽略掉一些冗余的信息。\n",
    "\n",
    "注意力机制中的Alignment Function译位对其函数,他的作用是通过一个类似对应关系的矩阵去找到输入于输出之间关系(更加官方的说法叫权重α)。\n",
    "\n",
    "在LLM进行学习, 例如 space 和 area 的翻译任务的时候, 会通过对齐函数来学习**一个特定的输出area下哪一个输入space对应的关联性最高**, 最终会形成一个矩阵的形式。\n",
    "\n",
    "不同的场景=不同的输入会学习并且生产不一样的Alignment Function。\n",
    "\n",
    "<img src=\"./assets/attention.png\" width=\"300\">\n",
    "\n",
    "## QKV in Attention\n",
    "\n",
    "- Q可以理解为当前用户的输入转换为向量的值\n",
    "- K可以理解为模型通过学习的结果把Q的特征给取出得到的一个向量的值\n",
    "- 通过Q和K计算出一个句子中每个词与词的相关性=权重=attention -> 对于注意力较高的Q和K的结果会给予更多的V（类似于Q中被关注的某个词的详细名片）\n",
    "\n",
    "以下GPT生成列子\n",
    "\n",
    "```markdown\n",
    "假设我们有一句话：“小明 喜欢 苹果”。在计算注意力时，我们先为每个词生成三个向量：Query、Key 和 Value。\n",
    "\n",
    "1. 生成向量\n",
    "\n",
    "每个词（例如“小明”、“喜欢”、“苹果”）通过各自的线性层生成 Q、K 和 V 向量。\n",
    "- Q 用来表示当前“关注点”，\n",
    "- k 用来表示每个词的“特征标签”，\n",
    "- V 则表示每个词的“实际信息内容”，例如“小明”的具体语义信息，“喜欢”的动作信息，“苹果”的物体信息。\n",
    "\n",
    "2. 计算注意力\n",
    "\n",
    "假设当前我们关注的词是“喜欢”，它的 Q 向量会与所有词的 K 向量做点积，得出每个词的相关性权重。\n",
    "比如，通过点积，模型可能计算出“苹果”对“喜欢”来说权重较高，因为在这句话里，“喜欢”通常与“苹果”的信息紧密关联。\n",
    "\n",
    "3. 加权求和得到输出\n",
    "\n",
    "最终，模型会根据这些权重，对所有词的 V 向量进行加权求和：\n",
    "\n",
    "\\text{输出} = \\text{权重}\\text{小明} \\times V{\\text{小明}} + \\text{权重}\\text{喜欢} \\times V{\\text{喜欢}} + \\text{权重}\\text{苹果} \\times V{\\text{苹果}}\n",
    "这里，“苹果”的 V 向量提供了关于苹果的实际内容（比如它代表的物体、颜色、大小等信息），因此如果“苹果”的权重最高，那么最终的输出就会更多地包含关于“苹果”的信息。\n",
    "\n",
    "总结：\n",
    "- V 向量 就像是每个词携带的“详细信息卡片”，在注意力机制中，这些信息卡片会被根据 Q 和 K 的匹配程度进行加权组合，最终形成输出。\n",
    "- 在这个过程中，Q 和 K 决定了“哪些信息更相关”，而 V 则是“实际被传递和整合的信息”。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Transformer ?\n",
    "\n",
    "- RNN ?\n",
    "  - 一种处理数据序列的神经网络\n",
    "  - 通过前一个的输出作为下一个学习的输入\n",
    "  - 但是效果会因为序列太长导致学习时会忽略太久远的内容（梯度爆照）\n",
    "    - attention 机制通过让模型在生成每个输出时“关注”整个输入序列，而不是仅依赖最后一个隐藏状态，从而缓解了长序列中的信息丢失问题。这种机制就像给模型提供了一种在所有输入中查找关键信息的能力 -> 通过对齐函数找到权重\n",
    "- Self-attention ?\n",
    "  - 一种应用于Transformer架构的注意力机制\n",
    "  - self-attention本身和attention没有太大的区别\n",
    "  - self-attention 在接受input的时候会计算embending的值从而可以让元素与元素之间更好的找到对应关系\n",
    "- encoding -> decoding ?\n",
    "  - 输入与输出\n",
    "- 学习语意本身\n",
    "  - transformer架构会通过encoding的次数去抽象多层使得transfomer结构能够学习语意本身\n",
    "  - RNN只能接受之前的传入,transformer能够在单词与单词之间寻找对应关系\n",
    "  - transformer 通常由多个编码器层和解码器层堆叠而成。每一层都进一步提炼和抽象特征\n",
    "    - 层数越多效果越好\n",
    "\n",
    "**总结一下 transformer 以及 self-attention**\n",
    "\n",
    "transformer架构中利用了self-attention机制让encoding的部分生成不同的**层**,每一层都会进行一系列的抽象操作作为下一层的输入,并且每一层都会提取更高层次的特征；decoding的部分会通过softmax把每个针对当前上下文生成下一个词的向量变为一个概率分布从而生成类似一个概率函数的东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT and BERT\n",
    "\n",
    "- 都是基于transformer架构\n",
    "- 两者都是通过无标注数据pre-training -> fine-tuning\n",
    "- 本质就是训练出一个类似\"概率分布\"的一个东西\n",
    "\n",
    "## BERT\n",
    "\n",
    "- 利用了transformer的encorder模块\n",
    "- 通过从左到右以及从右到左的双向上下文理解去学习\n",
    "- 通过[MASK]以及上句和下句的关联性去学习\n",
    "- 更适合做上下文理解的任务\n",
    "\n",
    "## GPT\n",
    "\n",
    "- 利用了transformer的decorder模块\n",
    "- 从左到右单向学习\n",
    "- 更适合做生成的任务\n",
    "\n",
    "**总结一下**\n",
    "\n",
    "GPT和BERT都会通过pre-traning得到一个基础的模型。\n",
    "\n",
    "基于这个基础的模型, 通过调整参数以及赋予特定任务的带标注的文本进行模型的调整用于对应的任务。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training model类型\n",
    "\n",
    "- encorder\n",
    "  - BERT\n",
    "- decorder\n",
    "  - gpt\n",
    "- encorder-decorder\n",
    "  - 结合encorder和decorder能力的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT FAMILY\n",
    "## GPT-1\n",
    "\n",
    "- 从transformer中独立出decorder的部分+大量数据训练\n",
    "\n",
    "## GPT-2\n",
    "\n",
    "- 简单的说就是在GPT-1的基础上加上更大的数据\n",
    "\n",
    "## GPT-3\n",
    "\n",
    "- 抛弃pre-training -> fine-tuning的范式, 直接一次性学习\n",
    "- 但是没有fine-tuning就没有很好的办法对应具体的task -> prompt的前身\n",
    "  - 给sample告诉模型我要干这个事\n",
    "\n",
    "## Prompt Engineering\n",
    "\n",
    "GPT-3中提出的一个概念。\n",
    "\n",
    "GPT-3模型中并没有用到pre-training -> fine-tuning的形式,而是进行了一次性的学习方式。\n",
    "\n",
    "这样的模型在处理特定的一些task中可以通过提供prompt来告诉模型应该怎么做,而不是通过fine-tuning的形式。\n",
    "\n",
    "## GPT-3.5\n",
    "\n",
    "- GPT3的基础上加上了coding训练 -> 让模型有了推理的能力\n",
    "- 除了coding训练, GPT3.5为了让模型理解更加广泛的人类问题, 使用了instruction-tuning的方式去大范围的适配各式各样的提问方式\n",
    "\n",
    "**总结一下GPT-3 / 3.5 之后的训练模式**\n",
    "\n",
    "GP3在通过无监督学习得到的pre-training模型后进行指令级别的微调以适应人类的提问。\n",
    "\n",
    "3.5通过有监督的学习 -> 通过人类去判断哪一个response is best的方式进行标注 -> 再通过PPO这个算法利用人类的反馈去进行强化学习。（强化学习不属于pre-training）\n",
    "\n",
    "## GPT-4\n",
    "\n",
    "- 支持了多模态的输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过GPT家族进行的prompt engineering的一些思考\n",
    "\n",
    "- 无论是1还是4,他们基于的一个底层逻辑并没有改变 -> 意味着该有的都有了。（数据意义上）\n",
    "- prompt engineering的必要性\n",
    "  - 简单任务不需要prompt engineering -> 本身模型就通过强化学习能够理解大部分的人类的各种各样的提问\n",
    "  - 复杂任务必须\n",
    "    - GPT-3.5之后的模型通过coding训练以及强化学习的微调让模型本身拥有了推理的能力\n",
    "\n",
    "**在底层的逻辑上, 强化学习过后的模型在接受人类的prompt的时候,一个好的prompt会更加敏感激活在被微调时的某一个特定的模式 -> 可以理解为“共鸣”**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Best Practice\n",
    "\n",
    "[第6章：AI大模型应用最佳实践.pdf](./assets/第6章：AI大模型应用最佳实践.pdf)\n",
    "\n",
    "- 用英语会效果会更好\n",
    "- 利用面向过程编程的效果会更好 -> step by step\n",
    "- system prompt的主要作用\n",
    "  - 解释主要任务\n",
    "  - 任务的拆解或者步骤\n",
    "  - 限制\n",
    "\n",
    "以上特性在GPT-4中会有体现。\n",
    "\n",
    "**那Gemini呢?**\n",
    "- 根据gpt的回答, gemini和gpt的训练->强化学习的部分是相通的,即使架构可能有些不同,但都是基于transformer\n",
    "- prompt engineering的做法同样适用于gemini"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-SbMP_ClQ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
